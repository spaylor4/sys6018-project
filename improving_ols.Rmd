---
title: "Improving Least Squares"
author: "Rachel Filderman, Latifa Hasan, Emily Murphy, Shannon Paylor"
date: "12/8/2020"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
    theme: cosmo
---

# Introduction

Have you recently mastered the introductory concepts in producing statistical models? Congratulations! With this knowledge, it’s now time to consider fine tuning model parameters further in order to improve the ordinary least squares (OLS) approach. One way we can do this is by determining if all of the parameters or variables are important in our model or if we instead can produce a better model after reducing the variables in the model through feature selection. You were likely introduced to subset selection in an introductory statistical model course, but there are other methods, as well, to improving OLS with feature selection. We will cover the following techniques in this tutorial with respect to the each method’s motivation and algorithm:

1. Subset Selection
    + Best Subsets
    + Stepwise Selection/Elimination
2. Penalized Regression
    + Ridge
    + Lasso
    + Elastic Net
3. Dimension Reduction
    + Principal Component Analysis (PCA)
    + Partial Least Squares (PLS)

In addition, we will demonstrate these techniques through a coding example with the following dataset.

Before we dive into dimension reduction, we will refresh a few key concepts.

### Review:
**Supervised vs Unsupervised Learning**

**Response Variable:** the variable of interest in a prediction problem, usually denoted $Y$.

**Features/predictor Variables:** data used in modeling, usually denoted $X$. In a supervised learning problem, these features are used to predict the response variable.

**Standardizing Variables:** re-scaling variables to have mean zero and variance one. Standardizing can be especially important when variables are on widely differing scales (e.g. one feature on a percentage scale with values between zero and one and another feature measured in dollars on a scale from zero to several thousand).

**Bias & Variance**

**Model Selection Criteria:** Model selection criteria are used to evaluate and compare subset regression models. Common criteria used include:
	+ Adjusted R^2 = [will write equation in latex/Rmd]
Akaike Information Criterion (AIC) = [will write equation in latex/Rmd]
	+ Bayesian Information Criterion (BIC) = [will write equation in latex/Rmd]
	+ Residual Sum of Squares (RSS) = [will write equation in latex/Rmd]
	
### Why improve on least squares?
OLS is not well-suited to high dimensional data. When the number of predictors $p$ is greater than the number of observations $n$, it will be possible to create a model explaining 100% of the variance in the data, which will lead to overfitting and poor prediction for new data.

### Real Life Applications:
+ DNA Sequencing
+ GDP 
+ Housing prices


# Subset Selection

# Penalized Regression

# Dimension Reduction

## Principal Components

## Partial Least Squares